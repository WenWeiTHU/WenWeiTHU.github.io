main:
  - title: "Sundial: A Family of Highly Capable Time Series Foundation Models"
    authors: <strong>Yong Liu*</strong>, Guo Qin*, Zhiyuan Shi, Zhi Chen, Caiyin Yang, Xiangdong Huang, Jianmin Wang, Mingsheng Long#
    conference_short: ICML
    conference: International Conference on Machine Learning, 2025.
    pdf: https://arxiv.org/abs/2502.00816
    image: ./assets/img/sundial_cover.png
    code: https://github.com/thuml/Sundial
    checkpoint: https://huggingface.co/thuml/sundial-base-128m
    poster: https://cloud.tsinghua.edu.cn/f/cc2a156315e9453f99b3/
    slides: https://cloud.tsinghua.edu.cn/f/8d526337afde465e87c9/
    intro: https://mp.weixin.qq.com/s/y3sc2e2lmW1sqfnoK-ZdDA
    notes: Oral Paper

  - title: "Timer: Generative Pre-trained Transformers Are Large Time Series Models"
    authors: <strong>Yong Liu*</strong>, Haoran Zhang*, Chenyu Li*, Xiangdong Huang, Jianmin Wang, Mingsheng Long#
    conference_short: ICML
    conference: International Conference on Machine Learning, 2024.
    pdf: https://arxiv.org/abs/2402.02368
    code: https://github.com/thuml/Large-Time-Series-Model
    image: ./assets/img/timer_cover.png
    poster: https://cloud.tsinghua.edu.cn/f/91da8a3d06984f209461/
    slides: https://cloud.tsinghua.edu.cn/f/b766629dbc584a4e8563/
    dataset: https://huggingface.co/datasets/thuml/UTSD
    intro: https://mp.weixin.qq.com/s/h9RHM_n89gjCohTXHu7m4Q
    notes: Accepted

  - title: "iTransformer: Inverted Transformers Are Effective for Time Series Forecasting"
    authors: <strong>Yong Liu*</strong>, Tengge Hu*, Haoran Zhang*, Haixu Wu, Shiyu Wang, Lintao Ma, Mingsheng Long#
    conference_short: ICLR
    conference: International Conference on Learning Representations, 2024.
    pdf: https://arxiv.org/abs/2310.06625
    code: https://github.com/thuml/iTransformer
    image: ./assets/img/itransformer_cover.png
    poster: https://cloud.tsinghua.edu.cn/f/36a2ae6c132d44c0bd8c/
    slides: https://cloud.tsinghua.edu.cn/f/175ff98f7e2d44fbbe8e/
    intro: https://mp.weixin.qq.com/s/-pvBnA1_NSloNxa6TYXTSg
    notes: Spotlight Paper, PaperDigest Most Influential Paper

  - title: "Non-stationary Transformers: Exploring the Stationarity in Time Series Forecasting"
    authors: <strong>Yong Liu*</strong>, Haiwu Wu*, Jianmin Wang, Mingsheng Long#
    conference_short: NeurIPS
    conference: Conference on Neural Information Processing Systems, 2022.
    pdf: https://arxiv.org/abs/2205.14415
    code: https://github.com/thuml/Nonstationary_Transformers
    image: ./assets/img/nsformer_cover.png
    poster: https://cloud.tsinghua.edu.cn/f/6eea66909aa7465ca9a4/
    slides: https://cloud.tsinghua.edu.cn/f/8d6ce7b18d3c468190e7/
    intro: https://mp.weixin.qq.com/s/LkpkTiNBVBYA-FqzAdy4dw
    notes: Accepted

  - title: "Timer-XL: Long-Context Transformers for Unified Time Series Forecasting"
    authors: <strong>Yong Liu*</strong>, Guo Qin*, Xiangdong Huang, Jianmin Wang, Mingsheng Long#
    conference_short: ICLR
    conference: International Conference on Learning Representations, 2025.
    pdf: https://arxiv.org/abs/2410.04803
    image: ./assets/img/timer_xl_cover.png
    code: https://github.com/thuml/Timer-XL
    lib: https://github.com/thuml/OpenLTM
    poster: https://cloud.tsinghua.edu.cn/f/378fbc6f0359460880aa/
    slides: https://cloud.tsinghua.edu.cn/f/2d4b660fc05148dc8f30/
    checkpoint: https://huggingface.co/thuml/timer-base-84m
    intro: https://mp.weixin.qq.com/s/IFqysOWo1prdjeBpCiNXBg
    notes: Accepted

  - title: "Koopa: Learning Non-stationary Time Series Dynamics with Koopman Predictors"
    authors: <strong>Yong Liu*</strong>, Chenyu Li*, Jianmin Wang, Mingsheng Long#
    conference_short: NeurIPS
    conference: Conference on Neural Information Processing Systems, 2023.
    pdf: https://arxiv.org/abs/2305.18803
    code: https://github.com/thuml/Koopa
    image: ./assets/img/koopa_cover.png
    poster: https://cloud.tsinghua.edu.cn/f/2f2fc7bd87d340ffaf29/
    slides: https://cloud.tsinghua.edu.cn/f/407ef231c6cb4727a6fa/
    intro: https://mp.weixin.qq.com/s/10PoA6n51Qok-nJT6_vkhA
    notes: Accepted

  - title: "TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis"
    authors: Haiwu Wu*, Tengge Hu*, <strong>Yong Liu*</strong>, Hang Zhou, Jianmin Wang, Mingsheng Long#
    conference_short: ICLR
    conference: International Conference on Learning Representations, 2023.
    pdf: https://arxiv.org/abs/2210.02186
    code: https://github.com/thuml/TimesNet
    image: ./assets/img/timesnet_cover.png
    slides: https://wuhaixu2016.github.io/pdf/ICLR2023_TimesNet.pdf
    intro: https://mp.weixin.qq.com/s/dhk-ASnrNG_Y99xykHaCUA
    notes: Accepted, PaperDigest Most Influential Paper

  - title: "CoRA: Covariate-Aware Adaptation of Time Series Foundation Models"
    authors: Guo Qin*, Zhi Chen*, <strong>Yong Liu*</strong>, Zhiyuan Shi, Haixuan Liu, Xiangdong Huang, Jianmin Wang, Mingsheng Long#
    conference_short: arxiv
    conference: arxiv preprint
    pdf: https://arxiv.org/pdf/2510.12681
    image: ./assets/img/cora_cover.png
    notes: arXiv preprint, 2025

  - title: "AutoTimes: Autoregressive Time Series Forecasters via Large Language Models"
    authors: <strong>Yong Liu*</strong>, Guo Qin*, Xiangdong Huang, Jianmin Wang, Mingsheng Long#
    conference_short: NeurIPS
    conference: Conference on Neural Information Processing Systems, 2024.
    pdf: https://arxiv.org/abs/2402.02370
    code: https://github.com/thuml/AutoTimes
    image: ./assets/img/autotimes_cover.png
    poster: https://cloud.tsinghua.edu.cn/f/f2c18ae34fef4e74ad46/
    slides: https://cloud.tsinghua.edu.cn/f/7689d30f92594ded84f0/
    notes: Accepted

  - title: "TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables"
    authors: Yuxuan Wang*, Haixu Wu*, Jiaxiang Dong, Guo Qin, Haoran Zhang, <strong>Yong Liu</strong>, Yunzhong Qiu, Jianmin Wang, Mingsheng Long#
    conference_short: NeurIPS
    conference: Conference on Neural Information Processing Systems, 2024.
    pdf: https://arxiv.org/abs/2402.19072
    code: https://github.com/thuml/TimeXer
    image: ./assets/img/timexer_cover.png
    notes: Accepted

  - title: "TimesBERT: A BERT-Style Foundation Model for Time Series Understanding"
    authors: Haoran Zhang*, <strong>Yong Liu*</strong>, Yunzhong Qiu*, Haixuan Liu, Zhongyi Pei, Jianmin Wang, Mingsheng Long
    conference_short: ACM MM
    conference: ACM International Conference on Multimedia, 2025.
    pdf: https://arxiv.org/abs/2502.21245
    image: ./assets/img/timesbert_cover.png
    notes: Accepted

  - title: "OLinear: A Linear Model for Time Series Forecasting in Orthogonally Transformed Domain"
    authors: Wenzhen Yue, <strong>Yong Liu</strong>, Haoxuan Li, Hao Wang, Xianghua Ying, Ruohao Guo, Bowei Xing, Ji Shi
    conference_short: NeurIPS
    conference: Conference on Neural Information Processing Systems, 2025.
    pdf: https://arxiv.org/abs/2505.08550
    image: ./assets/img/olinear_cover.png
    notes: Accepted

  - title: "FreEformer: Frequency Enhanced Transformer for Multivariate Time Series Forecasting"
    authors: Wenzhen Yue, <strong>Yong Liu</strong>, Xianghua Ying, Bowei Xing, Ruohao Guo, Ji Shi
    conference_short: IJCAI
    conference: International Joint Conferences on Artificial Intelligence, 2025.
    pdf: https://arxiv.org/abs/2501.13989
    image: ./assets/img/freeformer_cover.png
    notes: Accepted

  - title: "Deep Time Series Models: A Comprehensive Survey and Benchmark"
    authors: Yuxuan Wang*, Haixu Wu*, Jiaxiang Dong, <strong>Yong Liu</strong>, Mingsheng Long, Jianmin Wang#
    conference_short: arxiv
    conference: arxiv preprint
    pdf: https://arxiv.org/abs/2407.13278
    image: ./assets/img/dl4ts_cover.png
    notes: arXiv preprint, 2025

  - title: "Accuracy Law for the Future of Deep Time Series Forecasting"
    authors: Yuxuan Wang, Haixu Wu, Yuezhou Ma, Yuchen Fang, Ziyi Zhang, <strong>Yong Liu</strong>, Shiyu Wang, Zhou Ye, Yang Xiang, Jianmin Wang, Mingsheng Long#
    conference_short: arxiv
    conference: arxiv preprint
    pdf: https://arxiv.org/pdf/2510.02729
    image: ./assets/img/acclaw_cover.png
    notes: arXiv preprint, 2025

  - title: "Ranking and Tuning Pre-trained Models: A New Paradigm for Exploiting Model Hubs"
    authors: Kaichao You*, <strong>Yong Liu*</strong>, Ziyang Zhang, Jianmin Wang, Michael I. Jordan, Mingsheng Long#
    conference_short: JMLR
    conference: Journal of Machine Learning Research, 2022.
    pdf: https://arxiv.org/abs/2110.10545v4
    code: https://github.com/thuml/LogME
    image: ./assets/img/btuning_cover.png
    intro: https://mp.weixin.qq.com/s/fgmp5Cph8wIgf2IbPoHj7w
    notes: Accepted

  - title: "LogME: Practical Assessment of Pre-trained Models for Transfer Learning"
    authors: Kaichao You*, <strong>Yong Liu*</strong>, Jianmin Wang, Mingsheng Long#
    conference_short: ICML
    conference: International Conference on Machine Learning, 2021.
    pdf: https://proceedings.mlr.press/v139/you21b.html
    code: https://github.com/thuml/LogME
    image: ./assets/img/logme_cover.png
    intro: https://mp.weixin.qq.com/s/9lJEcwkXAN4jaENNghjpyw
    notes: Accepted
